{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3012cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'https://raw.githubusercontent.com/ek-chris/Practice_datasets/refs/heads/main/EasyVisa%20(1).csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a8f9567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Preprocessing libraries\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore, skew\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "def load_check_data_quality(file_path):\n",
    "    \"\"\"\n",
    "    Loads dataset, performs basic data quality check\n",
    "    \n",
    "    Steps:\n",
    "    1. Load CSV file.\n",
    "    2. check missing value.\n",
    "    3. check for duplicate.\n",
    "    4. Check skewness for variables identified in EDA as right-skewed.\n",
    "  \n",
    "    \"\"\"\n",
    "\n",
    "    # === 1. Load dataset ===\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Dataset loaded successfully: {file_path}\")\n",
    "        print(f\"Shape: {df.shape[0]} rows × {df.shape[1]} columns\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return None\n",
    "\n",
    "    # === 1. Missing Values ===\n",
    "    print(\"\\n=== MISSING VALUE PERCENTAGES ===\")\n",
    "    missing = df.isnull().mean() * 100\n",
    "    print(missing[missing > 0].sort_values(ascending=False))\n",
    "\n",
    "    # 2. Check for duplicates\n",
    "    print(\"\\n2. Duplicate Rows:\")\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"Number of duplicate rows: {duplicates}\")\n",
    "    if duplicates > 0:\n",
    "        print(f\"Percentage of duplicates: {(duplicates/len(df))*100:.2f}%\")\n",
    "\n",
    "    # 3. Check skewness for variables identified in EDA as right-skewed\n",
    "        print(\"\\n3. Skewness Analysis (EDA identified right-skewed variables):\")\n",
    "        skewed_vars = ['no_of_employees', 'yr_of_estab', 'prevailing_wage ']\n",
    "        for var in skewed_vars:\n",
    "            if var in df.columns:\n",
    "                skewness = skew(df[var])\n",
    "                print(f\"{var}: skewness = {skewness:.3f} \"\n",
    "                f\"({'strongly skewed' if abs(skewness) > 0.7 else 'moderately skewed' if abs(skewness) > 0.3 else 'approximately normal'})\")\n",
    "\n",
    "    # 4. Check correlation with target (EDA evidence)\n",
    "    # print(\"\\n4. Correlation with case_status (EDA Evidence):\")\n",
    "    # correlations = df.corr()['case_status'].sort_values(key=abs, ascending=False)\n",
    "    # print(\"High-signal features (|correlation| > 0.2):\")\n",
    "    # high_signal = correlations[abs(correlations) > 0.2].drop('case_status')\n",
    "    # for feature, corr in high_signal.items():\n",
    "    #     print(f\"  {feature}: {corr:.3f}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60e04fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully: https://raw.githubusercontent.com/ek-chris/Practice_datasets/refs/heads/main/EasyVisa%20(1).csv\n",
      "Shape: 25480 rows × 12 columns\n",
      "\n",
      "\n",
      "=== MISSING VALUE PERCENTAGES ===\n",
      "Series([], dtype: float64)\n",
      "\n",
      "2. Duplicate Rows:\n",
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "df = load_check_data_quality(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c27a3",
   "metadata": {},
   "source": [
    "Handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9d56f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OUTLIER TREATMENT (IQR-CAPPING METHOD) ===\n",
      "EDA recommended IQR-capping for  to preserve data points\n",
      "Treating outliers in 3 numerical features...\n",
      "\n",
      "Total outliers capped: 0\n",
      "Dataset shape after outlier treatment: (25480, 12)\n"
     ]
    }
   ],
   "source": [
    "# Outlier treatment based on EDA recommendations\n",
    "print(\"=== OUTLIER TREATMENT (IQR-CAPPING METHOD) ===\")\n",
    "print(\"EDA recommended IQR-capping for  to preserve data points\")\n",
    "\n",
    "# Define numerical columns (excluding target)\n",
    "numerical_cols =  ['no_of_employees', 'yr_of_estab', 'prevailing_wage']\n",
    "if 'Loan_Status' in numerical_cols:\n",
    "    numerical_cols.remove('Loan_Status')\n",
    "\n",
    "print(f\"Treating outliers in {len(numerical_cols)} numerical features...\")\n",
    "\n",
    "# Apply IQR-capping method\n",
    "outliers_capped = 0\n",
    "for col in numerical_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Count outliers before capping\n",
    "    outliers_before = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "    \n",
    "    if outliers_before > 0:\n",
    "        # Cap outliers\n",
    "        df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])\n",
    "        df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])\n",
    "        outliers_capped += outliers_before\n",
    "        print(f\"✓ {col}: Capped {outliers_before} outliers\")\n",
    "\n",
    "print(f\"\\nTotal outliers capped: {outliers_capped}\")\n",
    "print(f\"Dataset shape after outlier treatment: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61d1e6f",
   "metadata": {},
   "source": [
    " Encoding(like label encoding and one-hot encoding)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "274cc35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def preprocess_employee_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess the employee dataset by encoding categorical variables.\n",
    "    - Binary categorical columns: Label encoded using custom mappings\n",
    "    - Multi-category columns: One-hot encoded\n",
    "    \"\"\"\n",
    "\n",
    "    # Label Encoding (binary columns)\n",
    "    label_map = {\n",
    "        'has_job_experience': {'Y': 1, 'N': 0},\n",
    "        'requires_job_training': {'Y': 1, 'N': 0},\n",
    "        'full_time_position': {'Y': 1, 'N': 0},\n",
    "        'case_status': {'Certified': 1, 'Denied': 0}\n",
    "    }\n",
    "\n",
    "    for col, mapping in label_map.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map(mapping)\n",
    "\n",
    "    # One-hot Encoding (multi-category columns)\n",
    "    onehot_cols = [\n",
    "        'continent',\n",
    "        'education_of_employee',\n",
    "        'region_of_employment',\n",
    "        'unit_of_wage'\n",
    "    ]\n",
    "\n",
    "    df = pd.get_dummies(df, columns=onehot_cols, drop_first=False, dtype=int)\n",
    "\n",
    "    print(\"\\nPreprocessing complete.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a69f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing complete.\n",
      "  case_id  has_job_experience  requires_job_training  no_of_employees  \\\n",
      "0  EZYV01                   0                      0           7227.0   \n",
      "1  EZYV02                   1                      0           2412.0   \n",
      "2  EZYV03                   0                      1           7227.0   \n",
      "3  EZYV04                   0                      0             98.0   \n",
      "4  EZYV05                   1                      0           1082.0   \n",
      "\n",
      "   yr_of_estab  prevailing_wage  full_time_position  case_status  \\\n",
      "0       2007.0         592.2029                   1            0   \n",
      "1       2002.0       83425.6500                   1            1   \n",
      "2       2008.0      122996.8600                   1            0   \n",
      "3       1932.5       83434.0300                   1            0   \n",
      "4       2005.0      149907.3900                   1            1   \n",
      "\n",
      "   continent_Africa  continent_Asia  ...  education_of_employee_Master's  \\\n",
      "0                 0               1  ...                               0   \n",
      "1                 0               1  ...                               1   \n",
      "2                 0               1  ...                               0   \n",
      "3                 0               1  ...                               0   \n",
      "4                 1               0  ...                               1   \n",
      "\n",
      "   region_of_employment_Island  region_of_employment_Midwest  \\\n",
      "0                            0                             0   \n",
      "1                            0                             0   \n",
      "2                            0                             0   \n",
      "3                            0                             0   \n",
      "4                            0                             0   \n",
      "\n",
      "   region_of_employment_Northeast  region_of_employment_South  \\\n",
      "0                               0                           0   \n",
      "1                               1                           0   \n",
      "2                               0                           0   \n",
      "3                               0                           0   \n",
      "4                               0                           1   \n",
      "\n",
      "   region_of_employment_West  unit_of_wage_Hour  unit_of_wage_Month  \\\n",
      "0                          1                  1                   0   \n",
      "1                          0                  0                   0   \n",
      "2                          1                  0                   0   \n",
      "3                          1                  0                   0   \n",
      "4                          0                  0                   0   \n",
      "\n",
      "   unit_of_wage_Week  unit_of_wage_Year  \n",
      "0                  0                  0  \n",
      "1                  0                  1  \n",
      "2                  0                  1  \n",
      "3                  0                  1  \n",
      "4                  0                  1  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "df_processed = preprocess_employee_data(df)\n",
    "\n",
    "print(df_processed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c1a7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
