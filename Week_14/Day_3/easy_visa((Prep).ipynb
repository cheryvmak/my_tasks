{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8e7f8ce",
   "metadata": {},
   "source": [
    "### 1. Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "36bd0ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Preprocessing libraries\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Statistical libraries\n",
    "from scipy import stats\n",
    "from scipy.stats import zscore, skew\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3012cfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'https://raw.githubusercontent.com/ek-chris/Practice_datasets/refs/heads/main/EasyVisa%20(1).csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2985335e",
   "metadata": {},
   "source": [
    "### 2. EDA-Based Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "6a8f9567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_check_data_quality(file_path):\n",
    "    \"\"\"\n",
    "    Loads dataset, performs basic data quality check\n",
    "    \n",
    "    Steps:\n",
    "    1. Load CSV file.\n",
    "    2. check missing value.\n",
    "    3. check for duplicate.\n",
    "    4. Check skewness for variables identified in EDA as right-skewed.\n",
    "  \n",
    "    \"\"\"\n",
    "\n",
    "    # === 1. Load dataset ===\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Dataset loaded successfully: {file_path}\")\n",
    "        print(f\"Shape: {df.shape[0]} rows Ã— {df.shape[1]} columns\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return None\n",
    "\n",
    "    # === 1. Missing Values ===\n",
    "    print(\"\\n=== MISSING VALUE PERCENTAGES ===\")\n",
    "    missing = df.isnull().mean() * 100\n",
    "    print(missing[missing > 0].sort_values(ascending=False))\n",
    "\n",
    "    # 2. Check for duplicates\n",
    "    print(\"\\n2. Duplicate Rows:\")\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"Number of duplicate rows: {duplicates}\")\n",
    "    if duplicates > 0:\n",
    "        print(f\"Percentage of duplicates: {(duplicates/len(df))*100:.2f}%\")\n",
    "\n",
    "    # 3. Check skewness for variables identified in EDA as right-skewed\n",
    "    print(\"\\n3. Skewness Analysis (EDA identified right-skewed variables):\")\n",
    "    skewed_vars = ['no_of_employees', 'yr_of_estab', 'prevailing_wage ']\n",
    "    for var in skewed_vars:\n",
    "         if var in df.columns:\n",
    "            skewness = skew(df[var])\n",
    "         print(f\"{var}: skewness = {skewness:.3f} \"\n",
    "         f\"({'strongly skewed' if abs(skewness) > 0.7 else 'moderately skewed' if abs(skewness) > 0.3 else 'approximately normal'})\")\n",
    "\n",
    "   \n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "60e04fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully: https://raw.githubusercontent.com/ek-chris/Practice_datasets/refs/heads/main/EasyVisa%20(1).csv\n",
      "Shape: 25480 rows Ã— 12 columns\n",
      "\n",
      "\n",
      "=== MISSING VALUE PERCENTAGES ===\n",
      "Series([], dtype: float64)\n",
      "\n",
      "2. Duplicate Rows:\n",
      "Number of duplicate rows: 0\n",
      "\n",
      "3. Skewness Analysis (EDA identified right-skewed variables):\n",
      "no_of_employees: skewness = 12.265 (strongly skewed)\n",
      "yr_of_estab: skewness = -2.037 (strongly skewed)\n",
      "prevailing_wage : skewness = -2.037 (strongly skewed)\n"
     ]
    }
   ],
   "source": [
    "df = load_check_data_quality(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf22854",
   "metadata": {},
   "source": [
    "### 3. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "a3f4ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_employee_data(df):\n",
    "    \"\"\"\n",
    "    Data Cleaning\n",
    "    ---------------------\n",
    "    Handles duplicates, missing values, inconsistencies, and outliers.\n",
    "    \"\"\"\n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Handle missing values\n",
    "    print(\"\\n=== HANDLING MISSING VALUES ===\")\n",
    "\n",
    "    # lets iomport the imputer library\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    \n",
    "        # Prints and returns all categorical columns in the DataFrame.\n",
    "    numerical_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    print(f\"Numerical Features:\\n{numerical_features}\")\n",
    "\n",
    "    # Prints and returns all categorical columns in the DataFrame.\n",
    "    categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "    print(f\"Categorical Features:\\n{categorical_features}\")\n",
    "\n",
    "    # Lets create an instance of the imputer class using \"median\" as strategy for imputation\n",
    "    num_imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "    # Lets create an instance of the imputer class using \"most_frequent\" as strategy for imputation\n",
    "    cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "        # Lets apply the defined instances above\n",
    "    df[numerical_features] = num_imputer.fit_transform(df[numerical_features])\n",
    "    df[categorical_features] = cat_imputer.fit_transform(df[categorical_features])\n",
    "    \n",
    "    # Correct inconsistencies\n",
    "    text_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in text_cols:\n",
    "        df[col] = df[col].str.strip().str.title()\n",
    "\n",
    "    # Handle outliers (IQR method)\n",
    "    # Outlier treatment based on EDA recommendations\n",
    "    print(\"=== OUTLIER TREATMENT (IQR-CAPPING METHOD) ===\")\n",
    "    print(\"EDA recommended IQR-capping for  to preserve data points\")\n",
    "\n",
    "    # Define numerical columns (excluding target)\n",
    "    numerical_cols =  ['no_of_employees', 'yr_of_estab', 'prevailing_wage']\n",
    "    if 'case_status' in numerical_cols:\n",
    "        numerical_cols.remove('case_status')\n",
    "\n",
    "    print(f\"Treating outliers in {len(numerical_cols)} numerical features...\")\n",
    "\n",
    "    # Apply IQR-capping method\n",
    "    outliers_capped = 0\n",
    "    for col in numerical_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Count outliers before capping\n",
    "        outliers_before = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "        \n",
    "        if outliers_before > 0:\n",
    "            # Cap outliers\n",
    "            df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])\n",
    "            df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])\n",
    "            outliers_capped += outliers_before\n",
    "            print(f\"âœ“ {col}: Capped {outliers_before} outliers\")\n",
    "\n",
    "    print(f\"\\nTotal outliers capped: {outliers_capped}\")\n",
    "    print(f\"Dataset shape after outlier treatment: {df.shape}\")\n",
    "\n",
    "    print(\"Data cleaning complete.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f0115951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== HANDLING MISSING VALUES ===\n",
      "Numerical Features:\n",
      "['no_of_employees', 'yr_of_estab', 'prevailing_wage']\n",
      "Categorical Features:\n",
      "['case_id', 'continent', 'education_of_employee', 'has_job_experience', 'requires_job_training', 'region_of_employment', 'unit_of_wage', 'full_time_position', 'case_status']\n",
      "=== OUTLIER TREATMENT (IQR-CAPPING METHOD) ===\n",
      "EDA recommended IQR-capping for  to preserve data points\n",
      "Treating outliers in 3 numerical features...\n",
      "âœ“ no_of_employees: Capped 1556 outliers\n",
      "âœ“ yr_of_estab: Capped 3260 outliers\n",
      "âœ“ prevailing_wage: Capped 427 outliers\n",
      "\n",
      "Total outliers capped: 5243\n",
      "Dataset shape after outlier treatment: (25480, 12)\n",
      "Data cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "df = clean_employee_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ee73ed",
   "metadata": {},
   "source": [
    "### 4. Encoding(like label encoding and one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "cc4f63bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_employee_data(df):\n",
    "    \"\"\"\n",
    "    Step 2: Transformation (Encoding)\n",
    "    ---------------------------------\n",
    "    Converts categorical variables into numeric using Label & One-Hot encoding.\n",
    "    \"\"\"\n",
    "   # Label Encoding (binary columns)\n",
    "    label_map = {\n",
    "        'has_job_experience': {'Y': 1, 'N': 0},\n",
    "        'requires_job_training': {'Y': 1, 'N': 0},\n",
    "        'full_time_position': {'Y': 1, 'N': 0},\n",
    "        'case_status': {'Certified': 1, 'Denied': 0}\n",
    "    }\n",
    "\n",
    "    for col, mapping in label_map.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map(mapping)\n",
    "\n",
    "    # One-hot Encoding (multi-category columns)\n",
    "    onehot_cols = [\n",
    "        'continent',\n",
    "        'education_of_employee',\n",
    "        'region_of_employment',\n",
    "        'unit_of_wage'\n",
    "    ]\n",
    "\n",
    "    df = pd.get_dummies(df, columns=onehot_cols, drop_first=False, dtype=int)\n",
    "\n",
    "    print(\"Transformation (encoding) complete.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "2141b59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation (encoding) complete.\n",
      "  case_id  has_job_experience  requires_job_training  no_of_employees  \\\n",
      "0  Ezyv01                   0                      0           7227.0   \n",
      "1  Ezyv02                   1                      0           2412.0   \n",
      "2  Ezyv03                   0                      1           7227.0   \n",
      "3  Ezyv04                   0                      0             98.0   \n",
      "4  Ezyv05                   1                      0           1082.0   \n",
      "\n",
      "   yr_of_estab  prevailing_wage  full_time_position  case_status  \\\n",
      "0       2007.0         592.2029                   1            0   \n",
      "1       2002.0       83425.6500                   1            1   \n",
      "2       2008.0      122996.8600                   1            0   \n",
      "3       1932.5       83434.0300                   1            0   \n",
      "4       2005.0      149907.3900                   1            1   \n",
      "\n",
      "   continent_Africa  continent_Asia  ...  education_of_employee_Master'S  \\\n",
      "0                 0               1  ...                               0   \n",
      "1                 0               1  ...                               1   \n",
      "2                 0               1  ...                               0   \n",
      "3                 0               1  ...                               0   \n",
      "4                 1               0  ...                               1   \n",
      "\n",
      "   region_of_employment_Island  region_of_employment_Midwest  \\\n",
      "0                            0                             0   \n",
      "1                            0                             0   \n",
      "2                            0                             0   \n",
      "3                            0                             0   \n",
      "4                            0                             0   \n",
      "\n",
      "   region_of_employment_Northeast  region_of_employment_South  \\\n",
      "0                               0                           0   \n",
      "1                               1                           0   \n",
      "2                               0                           0   \n",
      "3                               0                           0   \n",
      "4                               0                           1   \n",
      "\n",
      "   region_of_employment_West  unit_of_wage_Hour  unit_of_wage_Month  \\\n",
      "0                          1                  1                   0   \n",
      "1                          0                  0                   0   \n",
      "2                          1                  0                   0   \n",
      "3                          1                  0                   0   \n",
      "4                          0                  0                   0   \n",
      "\n",
      "   unit_of_wage_Week  unit_of_wage_Year  \n",
      "0                  0                  0  \n",
      "1                  0                  1  \n",
      "2                  0                  1  \n",
      "3                  0                  1  \n",
      "4                  0                  1  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "df_processed = transform_employee_data(df)\n",
    "print(df_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9107ad4",
   "metadata": {},
   "source": [
    "### 5. Log-Transform Skewed Variables (EDA Recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "98c9b8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "\n",
    "def normalize_employee_data(df_processed):\n",
    "    \"\"\"\n",
    "    Step: Normalization\n",
    "    ---------------------\n",
    "    Safely applies log transformation to skewed numeric features.\n",
    "    Handles zeros, negatives, and NaNs automatically.\n",
    "    \"\"\"\n",
    "    print(\"=== LOG-TRANSFORMING SKEWED VARIABLES ===\")\n",
    "    print(\"EDA identified these variables as right-skewed and recommended log transformation:\")\n",
    "\n",
    "    # Variables identified as skewed from EDA\n",
    "    skewed_vars = ['no_of_employees', 'yr_of_estab', 'prevailing_wage']\n",
    "\n",
    "    for var in skewed_vars:\n",
    "        if var in df_processed.columns:\n",
    "            # Ensure numeric\n",
    "            if not np.issubdtype(df_processed[var].dtype, np.number):\n",
    "                print(f\"Skipping {var}: non-numeric column.\")\n",
    "                continue\n",
    "\n",
    "            # Handle NaN safely\n",
    "            df[var] = df_processed[var].fillna(0)\n",
    "\n",
    "            # Check for zeros/negatives\n",
    "            min_val = df_processed[var].min()\n",
    "            if min_val <= 0:\n",
    "                shift = abs(min_val) + 1\n",
    "                df_processed[f'{var}_log'] = np.log1p(df_processed[var] + shift)\n",
    "                print(f\"âœ“ {var}: Shifted by {shift:.2f} and applied log1p (min={min_val:.3f})\")\n",
    "            else:\n",
    "                df_processed[f'{var}_log'] = np.log1p(df_processed[var])\n",
    "                print(f\"âœ“ {var}: Applied log1p transformation (all positive values)\")\n",
    "\n",
    "            # Compute skewness safely\n",
    "            try:\n",
    "                original_skew = skew(df_processed[var], nan_policy='omit')\n",
    "                transformed_skew = skew(df_processed[f'{var}_log'], nan_policy='omit')\n",
    "                print(f\"  Original skewness: {original_skew:.3f} â†’ Transformed skewness: {transformed_skew:.3f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Skewness computation failed for {var}: {e}\")\n",
    "\n",
    "    print(f\"\\nDataset shape after log transformation: {df_processed.shape}\")\n",
    "    print(\"New log-transformed columns:\", [col for col in df_processed.columns if '_log' in col])\n",
    "    print(\"Normalization (log transform) complete.\\n\")\n",
    "\n",
    "    return df_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "f01d6387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOG-TRANSFORMING SKEWED VARIABLES ===\n",
      "EDA identified these variables as right-skewed and recommended log transformation:\n",
      "âœ“ no_of_employees: Shifted by 27.00 and applied log1p (min=-26.000)\n",
      "  Original skewness: 0.959 â†’ Transformed skewness: -1.145\n",
      "âœ“ yr_of_estab: Applied log1p transformation (all positive values)\n",
      "  Original skewness: -1.111 â†’ Transformed skewness: -1.124\n",
      "âœ“ prevailing_wage: Applied log1p transformation (all positive values)\n",
      "  Original skewness: 0.547 â†’ Transformed skewness: -2.136\n",
      "\n",
      "Dataset shape after log transformation: (25480, 30)\n",
      "New log-transformed columns: ['no_of_employees_log', 'yr_of_estab_log', 'prevailing_wage_log']\n",
      "Normalization (log transform) complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_processed = normalize_employee_data(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d7c9e",
   "metadata": {},
   "source": [
    "### 6. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "03357f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_employee_data(df_processed):\n",
    "    \"\"\"\n",
    "    Step 4: Feature Engineering\n",
    "    ---------------------------\n",
    "    Creates new, meaningful derived features.\n",
    "    \"\"\"\n",
    "    df_processed['wage_per_employee'] = df_processed['prevailing_wage'] / (df_processed['no_of_employees'] + 1)\n",
    "    df_processed['firm_age'] = 2025 - df_processed['yr_of_estab']\n",
    "    df['experience_training'] = df['has_job_experience'] * df_processed['requires_job_training']\n",
    "\n",
    "    print(\"Feature engineering complete.\")\n",
    "    print(f\"\\nDataset shape after feature engineering: {df_processed.shape}\")\n",
    "    print(f\"New engineered features: {[col for col in df_processed.columns if col not in df.columns]}\")\n",
    "\n",
    "    return df_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "53f00c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering complete.\n",
      "\n",
      "Dataset shape after feature engineering: (25480, 32)\n",
      "New engineered features: ['continent_Africa', 'continent_Asia', 'continent_Europe', 'continent_North America', 'continent_Oceania', 'continent_South America', \"education_of_employee_Bachelor'S\", 'education_of_employee_Doctorate', 'education_of_employee_High School', \"education_of_employee_Master'S\", 'region_of_employment_Island', 'region_of_employment_Midwest', 'region_of_employment_Northeast', 'region_of_employment_South', 'region_of_employment_West', 'unit_of_wage_Hour', 'unit_of_wage_Month', 'unit_of_wage_Week', 'unit_of_wage_Year', 'no_of_employees_log', 'yr_of_estab_log', 'prevailing_wage_log', 'wage_per_employee', 'firm_age']\n"
     ]
    }
   ],
   "source": [
    "df_processed = enhance_employee_data(df_processed)\n",
    "#df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683059f1",
   "metadata": {},
   "source": [
    "### 7. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "2d58b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_by_significance(df, p_threshold=0.05):\n",
    "    \"\"\"\n",
    "    Automatically keeps only statistically significant features (p < p_threshold)\n",
    "    based on Mannâ€“Whitney U (numeric) and Chi-square (categorical) tests.\n",
    "    Assumes 'case_status' is binary-encoded: 1 = Certified, 0 = Denied.\n",
    "    \"\"\"\n",
    "\n",
    "    from scipy.stats import mannwhitneyu, chi2_contingency\n",
    "\n",
    "    # Identify feature types\n",
    "    numeric_features = df.select_dtypes(include=['float64', 'int64']).columns.drop('case_status', errors='ignore')\n",
    "    categorical_features = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    significant_features = []\n",
    "\n",
    "    print(\"=== FEATURE SELECTION BASED ON STATISTICAL SIGNIFICANCE ===\")\n",
    "\n",
    "    # ---------- Numeric Features ----------\n",
    "    print(\"\\nðŸ”¹ Numeric Features (Mannâ€“Whitney U Test)\")\n",
    "    for feature in numeric_features:\n",
    "        group_cert = df[df['case_status'] == 1][feature].dropna()\n",
    "        group_denied = df[df['case_status'] == 0][feature].dropna()\n",
    "\n",
    "        # Skip columns with constant values\n",
    "        if group_cert.nunique() <= 1 or group_denied.nunique() <= 1:\n",
    "            print(f\"{feature}: Skipped (constant values)\")\n",
    "            continue\n",
    "\n",
    "        stat, p = mannwhitneyu(group_cert, group_denied, alternative='two-sided')\n",
    "\n",
    "        if p < p_threshold:\n",
    "            significant_features.append(feature)\n",
    "            print(f\"{feature}: p = {p:.4f}  Significant\")\n",
    "        else:\n",
    "            print(f\"{feature}: p = {p:.4f} Not significant\")\n",
    "\n",
    "    # ---------- Categorical Features ----------\n",
    "    print(\"\\nðŸ”¹ Categorical Features (Chi-square Test)\")\n",
    "    for feature in categorical_features:\n",
    "        contingency = pd.crosstab(df[feature], df['case_status'])\n",
    "        if contingency.shape[0] < 2 or contingency.shape[1] < 2:\n",
    "            print(f\"{feature}: Skipped (insufficient variation)\")\n",
    "            continue\n",
    "\n",
    "        chi2, p, _, _ = chi2_contingency(contingency)\n",
    "        if p < p_threshold:\n",
    "            significant_features.append(feature)\n",
    "            print(f\"{feature}: p = {p:.4f} Significant\")\n",
    "        else:\n",
    "            print(f\"{feature}: p = {p:.4f} Not significant\")\n",
    "\n",
    "    # ---------- Summary ----------\n",
    "    print(\"\\n=== SUMMARY ===\")\n",
    "    print(f\"Significant features retained ({len(significant_features)}): {significant_features}\")\n",
    "\n",
    "    # Keep only significant columns + target\n",
    "    selected_df = df[significant_features + ['case_status']].copy()\n",
    "\n",
    "    print(f\"Final dataset shape after feature selection: {selected_df.shape}\")\n",
    "    return selected_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "b242a913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE SELECTION BASED ON STATISTICAL SIGNIFICANCE ===\n",
      "\n",
      "ðŸ”¹ Numeric Features (Mannâ€“Whitney U Test)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has_job_experience: p = 0.0000  Significant\n",
      "requires_job_training: p = 0.1788 Not significant\n",
      "no_of_employees: p = 0.0000  Significant\n",
      "yr_of_estab: p = 0.0009  Significant\n",
      "prevailing_wage: p = 0.0000  Significant\n",
      "full_time_position: p = 0.0425  Significant\n",
      "experience_training: p = 0.0000  Significant\n",
      "\n",
      "ðŸ”¹ Categorical Features (Chi-square Test)\n",
      "case_id: p = 0.4971 Not significant\n",
      "continent: p = 0.0000 Significant\n",
      "education_of_employee: p = 0.0000 Significant\n",
      "region_of_employment: p = 0.0000 Significant\n",
      "unit_of_wage: p = 0.0000 Significant\n",
      "\n",
      "=== SUMMARY ===\n",
      "Significant features retained (10): ['has_job_experience', 'no_of_employees', 'yr_of_estab', 'prevailing_wage', 'full_time_position', 'experience_training', 'continent', 'education_of_employee', 'region_of_employment', 'unit_of_wage']\n",
      "Final dataset shape after feature selection: (25480, 11)\n"
     ]
    }
   ],
   "source": [
    "df_selected = feature_selection_by_significance(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d596e3c",
   "metadata": {},
   "source": [
    "### 8. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c50d463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified data splitting based on EDA findings about class imbalance\n",
    "print(\"=== STRATIFIED DATA SPLITTING ===\")\n",
    "print(\"EDA identified class imbalance - using stratified splitting to preserve class distribution\")\n",
    "\n",
    "# Select the chosen features\n",
    "X_selected = X[selected_features]\n",
    "print(f\"Selected features shape: {X_selected.shape}\")\n",
    "\n",
    "# First split: 80% train+val, 20% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_selected, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: 75% train, 25% validation (of the 80%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nData split results:\")\n",
    "print(f\"Training set: {X_train.shape} ({(X_train.shape[0]/len(X_selected))*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val.shape} ({(X_val.shape[0]/len(X_selected))*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape} ({(X_test.shape[0]/len(X_selected))*100:.1f}%)\")\n",
    "\n",
    "# Check class distribution in each set (should be similar due to stratification)\n",
    "print(f\"\\nClass distribution verification:\")\n",
    "print(\"Training set Loan_Status distribution:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(\"\\nValidation set Loan_Status distribution:\")\n",
    "print(y_val.value_counts().sort_index())\n",
    "print(\"\\nTest set Loan_Status distribution:\")\n",
    "print(y_test.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "c0e80b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection (significance testing) complete.\n"
     ]
    }
   ],
   "source": [
    "df = optimize_employee_features(df, target_col='case_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "dca45c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_employee_data(df, target_col='case_status', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Step 6: Data Splitting\n",
    "    ----------------------\n",
    "    Splits the dataset into train and test sets.\n",
    "    \"\"\"\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    print(\"Data splitting complete.\")\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "dc53b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_employee_features(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Step 7: Feature Scaling\n",
    "    -----------------------\n",
    "    Standardizes numeric features for model training.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    print(\"Feature scaling complete.\")\n",
    "    return X_train_scaled, X_test_scaled, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "e94ec9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def feature_importance_analysis(X_train_scaled, y_train, feature_names):\n",
    "    \"\"\"\n",
    "    Step 8: Feature Importance Analysis\n",
    "    -----------------------------------\n",
    "    Uses Random Forest to determine feature importance.\n",
    "    \"\"\"\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': rf.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    print(\"Feature importance analysis complete.\")\n",
    "    return importance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b5f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
